## Optimizers
- [Variants of Gradient Descent](http://ruder.io/optimizing-gradient-descent/)
## Regularization
 - [L1  (lasso) for dummies](https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a)

## Loss Functions

 - [Cross Entropy, KL Divergence](http://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/) 
 - [KL Divergence Forward and Reverse](https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/)

## Miscellaneous 
- [No Free Lunch Theorem](https://blog.yani.io/no-free-lunch/)
- [The VC (Vapnik-Chervonenkis\) Dimension](https://www.youtube.com/watch?v=Dc0sr0kdBVI&hd=1#t=3m24s)

## Basic Math concepts
- [Monte Carlo Approximation](https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/)
- [Positive Semidefinite Matrix](https://www.quora.com/What-is-a-positive-definite-matrix-in-laymans-terms)
- [Heteroscedastic / Heteroscedasticity model](http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html)
